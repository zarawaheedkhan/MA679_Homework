---
title: "Lab 6"
author: "Zara Waheed"
date: "March 12, 2022"
output: 
  pdf_document: 
    latex_engine: lualatex
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load("class","ISLR","glmnet","ggplot2","arm","knitr","data.table","MASS","klaR","vcd","h2o","e1071", "class", "boot", "leaps", "base", "pls", "gbm", "tree", "randomForest")
```


### Question 9.3

## a)

```{r}
x1 = c(3,2,4,1,2,4,4)
x2 = c(4,2,4,4,1,3,1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")

plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
```

## b)

```{r}
plot(x1, x2, col=colors, xlim=c(0,5), ylim=c(0,5))
abline(-0.5, 1)
```

## c)

Classify as red if 0.5 − X1 + X2 > 0
Classify as blue if 0.5 − X1 + X2 < 0

## d)

```{r}
plot(x1,x2,col=colors,xlim=c(0,5),ylim=c(0,5))
abline(-0.5, 1)
abline(-1, 1, lty=2)
abline(0, 1, lty=2)
```

## e)

```{r}
plot(x1, x2, col=colors, xlim=c(0,5), ylim=c(0,5))

abline(-0.5, 1)
arrows(2,1,2,1.5)
arrows(2,2,2,1.5)
arrows(4,4,4,3.5) 
arrows(4,3,4,3.5)
```

## f)

It would not have an effect on the maximal margin hyperplane since its not a support vector.

## g)

```{r}

plot(x1, x2, col=colors, xlim=c(0,5), ylim=c(0,5))
abline(-0.8, 1)
```

X1 + X2 > 0

## h)

```{r}
plot(x1, x2, col=colors, xlim=c(0,5), ylim=c(0,5))
points(c(5), c(1), col=c("red"))
```


### Question 9.5

## a)

```{r}
set.seed(100)
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1*(x1^2 - x2^2 > 0)
```

## b)

```{r}
plot(x1, x2, col = ifelse(y, "red", "blue"))
```

## c)

```{r}
df <- data.frame(x1, x2, y)
fit <- glm(y ~ x1 + x2, data = df, family = binomial)
```

## d)

```{r}
pred_fit <- predict(fit, data.frame(x1,x2))

plot(x1, x2, col = ifelse(pred_fit > 0, "red", "blue"), pch = ifelse(as.integer(pred_fit > 0) == y, 1,4))
```
circles: correctly classified
crosses: incorrectly classified

## e)

```{r, warning=FALSE}
fit1 <- glm(y ~ poly(x1, 2) + poly(x2, 2), data = df, family = binomial)
summary(fit1)
fit2 <- glm(y ~ x1 + x2 + x1*x2, data = df, family = binomial)
summary(fit2)
fit3 <- glm(y ~ x1 + x2 + log(x1) + log(x2), data = df, family = binomial)
summary(fit3)
```


## f)

```{r}
pred_fit <- predict(fit1, df)
plot(x1, x2, col = ifelse(pred_fit > 0, "red", "blue"), pch = ifelse(as.integer(pred_fit > 0) == y, 1,4))
```

## g)

```{r}
df$y <- as.factor(df$y)
fit_svc <- svm(y ~ x1 + x2, data = df, kernel = "linear")
pred_svc <- predict(fit_svc, df, type = "response")
plot(x1, x2, col = ifelse(pred_svc != 0, "red", "blue"), pch = ifelse(pred_svc == y, 1,4))
```

circles: correctly classified
crosses: incorrectly classified

## h)

```{r}
fit_svm <- svm(y ~ x1 + x2, data = df, kernel = "polynomial", degree = 2)
pred_svm <- predict(fit_svm, df, type = "response")
plot(x1, x2, col = ifelse(pred_svm != 0, "red", "blue"), pch = ifelse(pred_svm == y, 1,4))
```



## i)
Support vector model works better with a non-linear kernel and logistic regression with non-linear predictors.


### Question 9.7

##a)

```{r}
data("Auto")
Auto$Y <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto$Y <- as.factor(Auto$Y)
```

(b)

```{r}
set.seed(100)
costrange <- data.frame(cost = seq(0.01, 100, length.out = 10))
svm_auto <- tune(svm, Y ~ ., data = Auto, kernel = "linear", ranges = costrange)

summary(svm_auto)
plot(svm_auto$performances[,c(1,2)], type = "l")
```

Keeping the cost at 11.12 seems to perform the best.

(c)

```{r}
# Polynomial
costrange <- data.frame(cost = seq(0.01, 100, length.out = 5), degree = seq(1, 100, length.out = 5))
svm_poly_auto <- tune(svm, Y ~ ., data = Auto, kernel = "polynomial", ranges = costrange)
summary(svm_poly_auto)
```

Keeping the cost at 75 or 100 with degree 1 seems to perform the best.


```{r}
# Radial
costrange <- data.frame(cost=seq(0.01,100,length.out = 5),gamma=seq(0.1,100,length.out = 5))
svm_rad_auto <- tune(svm, Y ~ ., data = Auto, kernel = "radial", ranges = costrange)
summary(svm_rad_auto) 
```
Keeping a cost of 25 with gamma 0.1 seems to perform the best.

## d)

```{r}
fit1 <- svm(Y ~ ., data = Auto, kernel = "linear", cost = 11.12)
fit2 <- svm(Y ~ ., data = Auto, kernel = "polynomial", cost = 100, degree = 1)
fit3 <- svm(Y ~ ., data = Auto, kernel = "radial", cost = 25, gamma = 0.1)

svm_plot <- function(a){
  for (name in names(Auto)[!(names(Auto) %in% c("mpg", "Y", "name"))])
    plot(a, Auto, as.formula(paste("mpg~", name, sep = "")))
}

# got help from a classmate to create this function

svm_plot(fit1)

```

```{r}
svm_plot(fit2)
```

```{r}
svm_plot(fit3)
```

### Question 9.8

## a)

```{r}
data("OJ")
set.seed(100)
train_oj <- sample(nrow(OJ), 800)
train_OJ <- OJ[train_oj,]
test_OJ <- OJ[-train_oj,]
```

## b)

```{r}
svc_OJ <- svm(Purchase ~ ., data = train_OJ, kernel = "linear", cost = 0.01)
summary(svc_OJ)
```
432/800 support vectors were created

## c)

```{r}
set.seed(100)

pred_train_OJ <- predict(svc_OJ, train_OJ)
table(pred_train_OJ, train_OJ$Purchase)

pred_test_OJ <- predict(svc_OJ, test_OJ)
table(pred_test_OJ, test_OJ$Purchase)
```

Training error rate = 55+78/433+78+55+234 = 16.63% 
Test error rate = 26+18/147+26+18+79 = 16.30%

## d)

```{r}
tune_OJ <- tune(svm, Purchase ~ ., data = train_OJ, kernel = "linear", ranges = data.frame(cost = seq(0.01, 10, length.out = 30)))
summary(tune_OJ)
```
0.7 cost seems to lower the error the most.

## e)

```{r}
set.seed(100)

svm_OJ <- svm(Purchase ~ ., data = train_OJ, kernel = "linear", cost = tune_OJ$best.parameters$cost)

svm_train <- predict(svm_OJ, train_OJ)
table(svm_train, train_OJ$Purchase)

svm_test <- predict(svm_OJ, test_OJ)
table(svm_test, test_OJ$Purchase)
```

Training error rate = 66+61/427+66+61+246 = 15.88%
Test error rate = 27+22/143+27+22+78 = 18.15%.

## f)

```{r}
set.seed(100)

svm_radial_OJ <- svm(Purchase ~ ., data = train_OJ, kernel = "radial")
summary(svm_radial_OJ)

svm_radial_train <- predict(svm_radial_OJ, train_OJ)
table(svm_radial_train, train_OJ$Purchase)

svm_radial_test <- predict(svm_radial_OJ, test_OJ)
table(svm_radial_test, test_OJ$Purchase)
```
Training error rate = 69+40/448+69+40+243 = 13.63% 
Test error rate = 32+18/147+32+18+73 = 18.52%

```{r}

# Tuning - Find cost

svm_radial_tune_OJ <- tune(svm, Purchase ~ ., data = train_OJ, kernel = "radial", ranges = data.frame(cost = seq(0.01, 10, length.out = 30)))
summary(svm_radial_tune_OJ)
```

The optimal cost is around 1

```{r}
# Tuning - Find errors

set.seed(100)

svm_radial_OJ <- svm(Purchase ~ ., data = train_OJ, kernel = "radial", cost = svm_radial_tune_OJ$best.parameters$cost)

svm_radial_train <- predict(svm_radial_OJ, train_OJ)
table(svm_radial_train, train_OJ$Purchase)

svm_radial_test <- predict(svm_radial_OJ, test_OJ)
table(svm_radial_test, test_OJ$Purchase)
```

Training error rate = 71+43/445+71+43+241 = 14.25% 
Test error rate = 32+18/147+32+18+73 = 18.52%

## g)

```{r}
poly_OJ <- svm(Purchase ~ ., data = train_OJ, kernel = "polynomial", degree = 2)
summary(poly_OJ)
```
447/800 support vectors created.

```{r}
set.seed(100)

svm_poly_train <- predict(poly_OJ, train_OJ)
table(svm_poly_train, train_OJ$Purchase)

svm_poly_test <- predict(poly_OJ, test_OJ)
table(svm_poly_test, test_OJ$Purchase)
```

Training error rate = 105+30/458+105+30+207 = 16.88% 
Test error rate = 45+13/152+45+13+60 = 21.48%

```{r}
# Tuning - Find cost
svm_poly_tune <- tune(svm, Purchase ~ ., data = train_OJ, kernel = "polynomial", degree = 2, ranges =
                  data.frame(cost = seq(0.01, 10, length.out = 30)))
summary(svm_poly_tune)
```

The optimal cost is around 7.6

```{r}
# Tuning - Find errors

set.seed(100)

svm_poly_tune <- svm(Purchase ~ ., data = train_OJ, kernel = "polynomial", cost = svm_poly_tune$best.parameters$cost)

poly_train <- predict(svm_poly_tune, train_OJ)
table(poly_train, train_OJ$Purchase)

poly_test <- predict(svm_poly_tune, test_OJ)
table(poly_test, test_OJ$Purchase)
```

Training error rate = 72+36/452+72+36+240 = 13.50% 
Test error rate = 35+20/145+35+20+70 = 20.37%

## h)

linear kernel SVM 0.7 cost gives the lowest error rate which is 18.15%
