---
title: "Lab 3"
author: "Zara Waheed"
date: "15th Feb 2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=6,fig.height=4 ,out.width="1\\linewidth")
pacman::p_load("class","ISLR","glmnet","ggplot2","arm","knitr","data.table","MASS","klaR","vcd","h2o","e1071", "class", "boot", "leaps", "base", "pls")
```

### Question 5.8

## a)

```{r}
set.seed(100)
x = rnorm(100)
y = x - 2*x^2 + rnorm(100)
```

n = 100 
p = 2
Y= X − 2X^2 + ϵ

## b) 

```{r}
plot(x, y)
```

The equation is quadratic as can be seen from the plot and the approximate ranges of x and y are -2 to 2 and -8 to 2, respectively.

## c) 

```{r}

df = data.frame(x, y)
set.seed(100)

# i) Y = B0 + B1*X + ϵ
fit1 = glm(y ~ x)
cv.glm(df, fit1)$delta

# ii) Y = B0 + B1*X + B2*X^2 + ϵ
fit2 = glm(y ~ poly(x, 2))
cv.glm(df, fit2)$delta

# iii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + ϵ
fit3 = glm(y ~ poly(x, 3))
cv.glm(df, fit3)$delta 

# iv) Y = B0 + B1*X + B2*X^2 + B3*X^3 + B4*X^4 + ϵ
fit4 = glm(y ~ poly(x, 4))
cv.glm(df, fit4)$delta

```


## d) 

```{r}
set.seed(100)

# i) Y = B0 + B1*X + ϵ
fit5 = glm(y ~ x)
cv.glm(df, fit5)$delta

# ii) Y = B0 + B1*X + B2*X^2 + ϵ
fit6 = glm(y ~ poly(x, 2))
cv.glm(df, fit6)$delta

# iii) Y = B0 + B1*X + B2*X^2 + B3*X^3 + ϵ
fit7 = glm(y ~ poly(x, 3))
cv.glm(df, fit7)$delta 

# iv) Y = B0 + B1*X + B2*X^2 + B3*X^3 + B4*X^4 + ϵ
fit8 = glm(y ~ poly(x, 4))
cv.glm(df, fit8)$delta

```

The results are exactly the same as part c).

## e) 
Equation ii) had the lowest error rate, which could be because it is in quadratic form and is similar to the y equation.

## f)

```{r}
summary(fit1)
```

Yes these results agree with the cross validation results.

### Question 6.9 

## a)

```{r}
data(College)
set.seed(1)
df1 <- sample(1:dim(College)[1], dim(College)[1] / 2)
df2 <- -df1
train <- College[df1, ]
test <- College[df2, ]
```


## b)
```{r}
fit9b <- lm(Apps ~ ., data = train)
pred.lm <- predict(fit9b, test)
mean((pred.lm - test$Apps)^2)
```


## c)

```{r}
set.seed(100)

train.mx <- model.matrix(Apps ~., data = train[ ,-1])
test.mx <- model.matrix(Apps ~., data = test[ ,-1])
cv.ridge <- cv.glmnet(train.mx, train$Apps, alpha = 0)
lambda.ridge <- cv.ridge$lambda.min


pred.ridge <- predict(cv.ridge, s = lambda.ridge, newx = test.mx)
mean((pred.ridge - test$Apps) ^2)
lambda.ridge
```


## d)

```{r}
set.seed(1)
cv.lasso <- cv.glmnet(train.mx, train$Apps, alpha = 1)
lambda.lasso <- cv.lasso$lambda.min
lambda.lasso
pred.lasso <- predict(cv.lasso, s = lambda.lasso, newx = test.mx)
mean((pred.lasso - test$Apps) ^2)
```

## e)

```{r}
pcr.fit <- pcr(Apps ~ ., data = train, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")

pred.pcr <- predict(pcr.fit, test.mx, ncomp = 5)
mean((pred.pcr - test$Apps)^2)
```


## f)

```{r}
set.seed(100)

pls.fit <- plsr(Apps ~ ., data = train, scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
pred.pls <- predict(pls.fit, test.mx, ncomp = 10)
mean((pred.pls - test$Apps) ^2)
```

## g)

```{r}

# Calculate R^2 for all models

test.avg <- mean(test$Apps)
lm <- 1- mean((pred.lm - test$Apps)^2) / mean((test.avg - test$Apps)^2)
ridge <- 1- mean((pred.ridge - test$Apps)^2) / mean((test.avg - test$Apps)^2)
lasso <- 1- mean((pred.lasso - test$Apps)^2) / mean((test.avg - test$Apps)^2)
pcr <- 1- mean((pred.pcr - test$Apps)^2) / mean((test.avg - test$Apps)^2)
pls <- 1- mean((pred.pls - test$Apps)^2) / mean((test.avg - test$Apps)^2)

lm
ridge
lasso
pcr
pls
```

All of the models are fairly accurate except PCR


### Question 6.10

## a)

```{r}
set.seed(100)

p = 20
n = 1000
x = matrix(rnorm(n*p),n,p)
B = rnorm(p) 
B[c(2,3,8,9,10,15,20)] = 0 
e = rnorm(n)
y = x %*% B + e
```


## b) 

```{r}
data <- data.frame(x, y)
#train = sample(seq(1000),100,replace=F)
train <- data[1:100,]
test <- data[101:1000,]
```

## c) 

```{r}
subset = regsubsets(y~.,train, nvmax=p)
plot(summary(subset)$rss/100)
```

## d)

```{r}
test.mx <- model.matrix(y ~., test, nvmax = 20)
errors <- rep(NA, 20)
for (i in 1:20) {
  coef <- coef(subset, id = i)
  pred <- test.mx[, names(coef)] %*%coef
  errors[i] <- mean((pred - test[,21])^2) 
}
plot(errors, xlab = "Variables", ylab = "MSE", type = "b", pch = 20)
axis(1, at = seq(1, 20, 1))
```

## e) 

```{r}
which.min(errors)
```


## f)

```{r}
coef(subset, which.min(errors))
```


## g)

```{r}
errors <- rep(NA, 20)
x_colname <- colnames(x, do.NULL = FALSE, prefix = "X")
for (i in 1:20) {
  coeff <- coef(subset, id = i)
  errors[i] <- sqrt(sum((B[x_colname %in% names(coeff)] - coeff[names(coeff) %in% x_colname])^2) + sum(B[!(x_colname %in% names(coeff))])^2) 
}
plot(errors, xlab = "variables", ylab = "MSE", type = "b", pch = 19)
axis(1, at = seq(1, 20, 1))
```

The plot shows a drop in the coefficient error.